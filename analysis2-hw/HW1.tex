%!TEX encoding = utf-8
\documentclass[12pt]{report}
\usepackage{kotex}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{geometry}
\geometry{
    top = 20mm,
    left = 20mm,
    right = 20mm,
    bottom = 20mm
}
\geometry{a4paper}

\pagenumbering{gobble}
\renewcommand{\baselinestretch}{1.4}
\newcommand{\numl}[1]{\item[\large\textbf{\sffamily #1.}]}
\newcommand{\num}[1]{\item[\textbf{\sffamily #1}]}

\newcommand{\ds}{\displaystyle}

\newcommand{\mf}[1]{\mathfrak{#1}}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\bb}[1]{\mathbb{#1}}
\newcommand{\rmbf}[1]{\mathrm{\mathbf{#1}}}

\newcommand{\inv}{^{-1}}
\newcommand{\adj}{\text{*}}
\newcommand{\bs}{\setminus}
\renewcommand{\subset}{\subseteq}

\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}
\newcommand{\abs}[1]{\left| #1 \right|}
\newcommand{\paren}[1]{\left( #1 \right)}
\newcommand{\seq}[1]{\left\{ #1 \right\}}
\renewcommand{\span}[1]{\left\langle #1 \right\rangle}

\newcommand{\ra}{\rightarrow}
\newcommand{\uc}{\overset{u}{\ra}}
\newcommand{\imp}{\implies}
\newcommand{\mimp}{\(\implies\)}
\newcommand{\mimpd}{\(\impliedby\)}
\newcommand{\miff}{\!\!\(\iff\)}

\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}

\newcommand{\inte}{\mathrm{int}}
\newcommand{\diam}{\text{diam}}
\newcommand{\dist}{\text{dist}}
\newcommand{\lint}[2]{\underline{\int_{#1}^{#2}}}
\newcommand{\uint}[2]{\overline{\int_{#1}^{#2}}}
\renewcommand{\d}[1]{\,d{#1}}

\let\oldexists\exists
\renewcommand{\exists}{\oldexists\,}

\begin{document}
\begin{center}
    \textbf{\Large 해석개론 및 연습 2 과제 \#1}\\
    \large 2017-18570 컴퓨터공학부 이성찬
\end{center}
\begin{enumerate}
    \numl{1} Suppose that \(\seq{f_n}_{n=1}^\infty\) converges uniformly to \(f\) on \(X\), and that \(\abs{f_n} \leq M_n\) for all \(n \in \N\).

    By uniform convergence, we can choose \(N \in \N\) such that for \(n \geq N\),
    \[
        \abs{f_n(x) - f(x)} < 1, \quad \forall x \in X.
    \]
    Thus, for \(x \in X\) and \(n \geq N\), we can write
    \[
        \abs{f_n(x)} \leq \abs{f_n(x) - f(x)} + \abs{f(x) - f_N(x)} + \abs{f_N(x)} < 2 + M_N.
    \]
    Now set \(M = \max\{M_1, M_2, \dots, M_{N-1}, 2 + M_N\}\). Then for all \(n \in \N\),
    \[
        \abs{f_n(x)} \leq M,
    \]
    which shows that \(\seq{f_n}\) is uniformly bounded.

    \numl{2} Suppose that \(f_n \ra f\), \(g_n \ra g\) uniformly on \(E\), and let \(\epsilon > 0\) be given. By uniform convergence of \(f_n\), \(g_n\), we can choose \(N_1, N_2 \in \N\) such that
    \begin{center}
        \(n \geq N_1 \implies \abs{f_n(x) - f(x)} < \dfrac{\epsilon}{2}\) and \(n \geq N_2 \implies \abs{g_n(x) - g(x)} < \dfrac{\epsilon}{2}\)
    \end{center}
    for all \(x \in E\). Set \(N = \max\{N_1, N_2\}\), we find that for \(n \geq N\),
    \[
        \abs{f_n(x) + g_n(x) - \bigl(f(x) + g(x)\bigr)} \leq \abs{f_n(x) - f(x)} + \abs{g_n(x) - g(x)} < \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon
    \]
    for all \(x \in E\). Thus \(f_n + g_n\) converges uniformly to \(f + g\) on \(E\).

    If \(f_n\), \(g_n\) are bounded, we know that they are both uniformly bounded by the first problem. Additionally, we know that \(f\) is bounded by {\sffamily Theorem 7.15}. Thus there exists \(F, G \in \R \bs \{0\}\) such that \(\abs{f_n(x)} \leq F\), \(\abs{f(x)} \leq F\) and \(\abs{g_n(x)} \leq G\), for all \(x \in E\).

    Let \(\epsilon > 0\) be given. Using the uniform convergence of \(f_n\) and \(g_n\), we can choose \(M_1, M_2 \in \N\) such that
    \begin{center}
        \(n \geq M_1 \implies \abs{f_n(x) - f(x)} < \dfrac{\epsilon}{2G}\) and \(n \geq M_2 \implies \abs{g_n(x) - g(x)} < \dfrac{\epsilon}{2F}\)
    \end{center}
    for all \(x \in E\). Set \(M = \max\{M_1, M_2\}\), we find that for \(n \geq M\),
    \[
        \begin{aligned}
            \abs{f_n(x)g_n(x) - f(x)g(x)} & = \abs{f_n(x)g_n(x) - f(x)g_n(x) + f(x)g_n(x) - f(x)g(x)}                                                            \\
                                          & \leq \abs{g_n(x)}\abs{f_n(x) - f(x)} + \abs{f(x)} \abs{g_n(x) - g(x)}                                                \\
                                          & \leq G \cdot \frac{\epsilon}{2G} + F \cdot \frac{\epsilon}{2F} = \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon,
        \end{aligned}
    \]
    for all \(x \in E\). Thus \(f_ng_n\) converges uniformly to \(fg\) on \(E\).

    \numl{3} It is easy to see that \(f_n \ra f \equiv 0\), making \(f\) a continuous function. But the convergence is not uniform. For instance, take \(\epsilon = 1/2\). For all \(n \in \N\), there exists some \(x\in \R\) such that
    \[
        \abs{f_n(x) - f(x)} = \abs{f_n(x)} \geq \frac{1}{2}.
    \]
    \(x_0 = \dfrac{1}{n + 1/2}\) is such \(x\), because
    \[
        \abs{f_n(x_0)} = \sin^2 \dfrac{\pi}{x_0} = \sin^2 \left(n\pi + \frac{\pi}{2}\right) = 1 \geq \frac{1}{2}.
    \]

    Now we calculate \(\sum f_n(x)\). For \(x \leq 0\), \(x \geq 1\), \(\sum f_n(x) = 0\).

    For \(x \in (0, 1)\),
    \begin{enumerate}
        \item If \(x = \dfrac{1}{N}\) for some \(N \in \N\), \(f_n(x) = 0\) for all \(n \in \N\).
        \item Otherwise, \(\exists N \in \N\) such that \(\ds \frac{1}{N + 1} < x < \frac{1}{N}\). Then
              \[
                  f_n(x) = \begin{cases}
                      \sin^2 \dfrac{\pi}{x} & (n = N)    \\
                      0                     & (n \neq N)
                  \end{cases}.
              \]
    \end{enumerate}
    Thus, \(\sum f_n(x) = \sin^2 \dfrac{\pi}{x}\) for \(x \in (0, 1)\). Overall,
    \[
        f(x) = \sum f_n(x) = \begin{cases}
            \sin^2 \dfrac{\pi}{x} & (x \in (0, 1))     \\
            0                     & (\text{otherwise})
        \end{cases}.
    \]

    Since all the terms are non-negative, the series converges absolutely.

    If \(\sum f_n(x)\) were to converge uniformly to \(f\), \(f\) should have been continuous. But since \(f\) is not continuous at \(x = 0\), \(\sum f_n(x)\) cannot converge uniformly.

    \numl{4} The given series can be considered as the sum of the two following series
    \[
        A(x) = x^2 \sum_{n=1}^\infty \frac{(-1)^n}{n^2}, \quad B = \sum_{n=1}^\infty \frac{(-1)^n}{n}
    \]
    because both \(A(x)\) and \(B\) converge, by the alternating series test.

    Let \(a > 0\) and set \(X = [-a, a]\). It is sufficient to show uniform convergence for \(X\), because \(a\) can be chosen arbitrarily large so that it would contain any bounded interval.

    \(A(x)\) converges uniformly on \(X\) by using the Weierstrass \(M\)-test with \(M_n = \dfrac{a^2}{n^2}\). However, the given series does not converge absolutely by the comparison test since
    \[
        \abs{(-1)^n \frac{x^2 + n}{n^2}} \geq \frac{n}{n^2} = \frac{1}{n}
    \]
    and the harmonic series diverges.

    \numl{5} Since \(f_n(0) = 0\), \(f(0) = 0\). Now consider the case \(x \neq 0\). For \(\epsilon > 0\), choose \(N = \dfrac{1}{4\epsilon^2}\). Then for \(n \geq N\),
    \[
        \abs{f_n(x) - 0} = \frac{1}{\abs{nx + \dfrac{1}{x}}} \leq \frac{1}{2\sqrt{n}} \leq \frac{1}{2\sqrt{N}} = \epsilon, \quad (x \in \R\bs\{0\})
    \]
    (AM-GM inequality was used in the first inequality) Thus \(f(x) = 0\) also for \(x \neq 0\). \(f_n\) converges uniformly to \(f(x) = 0\) for \(x \in \R\).

    We directly calculate \(f_n'(x)\) and get
    \[
        f_n'(x) = \frac{1 - nx^2}{(1 + nx^2)^2}.
    \]
    We see that
    \[
        \lim_{n\ra\infty} f_n'(x) = \begin{cases}
            1 & (x = 0) \\ 0 & (x \neq 0)
        \end{cases},
    \]
    whereas \(f'(x) = 0\). The given equation \(f'(x) = \ds \lim_{n \ra \infty} f'_n(x)\) is false only for \(x = 0\).

    \numl{6} We directly see that
    \[
        \abs{c_n I(x - x_n)} \leq \abs{c_n}
    \]
    and since \(\sum \abs{c_n} < \infty\), the given series converges uniformly on \([a, b]\) by the Weierstrass \(M\)-test.

    Define the partial sum \(s_n(x) = \ds \sum_{k =1}^n c_k I(x - x_k)\). We know that \(s_n(x)\) is already continuous at \(x_0 \neq x_n\), since each term in the sum is continuous at \(x_0\) and the sum is finite. Thus
    \[
        \lim_{t \ra x_0} s_n(t) = s_n(x_0).
    \]
    Since \(s_n(x)\) converges uniformly to \(f(x)\), \(\seq{s_n(x_0)}\) converges to \(f(x_0)\).

    By {\sffamily Theorem 7.11}, (the conditions for the theorem are indeed satisfied)
    \[
        \lim_{t \ra x_0} f(t) = \lim_{n\ra\infty} \lim_{t \ra x_0} s_n(t) = \lim_{n\ra\infty} s_n(x_0) = f(x_0),
    \]
    showing that \(f\) is continuous for every \(x_0 \neq x_n\).

    \numl{7} We are given that \(\seq{f_n}\) is a sequence of continuous functions converging uniformly to \(f\). We know that \(f\) is continuous on \(E\). Let \(\epsilon > 0\) be given.

    First, we choose \(N_1 \in \N\) such that for \(n \geq N_1\),
    \[
        \abs{f_n(x) - f(x)} < \frac{\epsilon}{2}, \quad \forall x \in E.
    \]
    Next, since \(f_n\) is continuous at \(x \in E\), (the limit of \(x_n\)) there exists \(\delta > 0\) such that
    \[
        \abs{y - x} < \delta \implies \abs{f_n(y) - f_n(x)} < \frac{\epsilon}{2}, \quad \forall y\in E.
    \]
    Lastly, since \(x_n \ra x\), we choose choose \(N_2 \in \N\) such that for \(n \geq N_2\),
    \[
        \abs{x_n - x} < \delta,
    \]
    which implies that
    \[
        \abs{f_n(x_n) - f_n(x)} < \frac{\epsilon}{2}.
    \]

    Therefore, for \(n \geq \max\{N_1, N_2\}\),
    \[
        \abs{f_n(x_n) - f(x)} \leq \abs{f_n(x_n) - f_n(x)} + \abs{f_n(x) - f(x)} < \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon,
    \]
    which is equivalent to \(\ds \lim_{n\ra\infty} f_n(x_n) = f(x)\).

    The converse is not true. Consider the function \(f_n\) given in {\sffamily Problem \#3}, with the domain restricted to \(E = [0, 1]\). First of all, \(f_n\) does not converge uniformly to \(f \equiv 0\) on \(E\).

    Now, consider any sequence \(\seq{x_n} \ra x \in E\). For very small \(\epsilon > 0\) such that \(x - \epsilon > 0\), choose \(N \in \N\) such that for \(n \geq N_1\), \(\abs{x_n - x} < \epsilon\). Then, we see that for \(n \geq N_1\), \(0 < x - \epsilon < x_n\).

    Therefore we can choose \(M\) large enough so that \(x - \epsilon > \dfrac{1}{M}\). Then \(f_M(x - \epsilon) = 0\). Setting \(N = \max\{N_1, M\}\) will give \(f_n(x_n) = 0\) for \(n \geq N\).

    Hence, \(\ds \lim_{n\ra\infty} f_n(x_n) = f(x)\) holds, but \(f_n\) does not converge uniformly.


    \numl{8} First we prove the following lemma.

    \quad {\sffamily \bfseries Lemma.} \textit{Given two sequences \(\seq{a_n}, \seq{b_n}\) and a partial sum \(A_n = \ds\sum_{k=1}^n a_k\), (define \(A_0 = 0\)) the following holds for \(m < n\).
        \begin{equation} \tag{\textasteriskcentered}
            \sum_{k=m+1}^n a_n b_n = A_n b_{n+1} - A_m b_{m+1} - \sum_{k=m+1}^n A_k \bigl(b_{k+1} - b_k\bigr)
        \end{equation}
    }

    \quad {\sffamily \bfseries Proof of Lemma.} Observe that
    \[
        \begin{aligned}
            \sum_{k=1}^n a_k b_k & = \sum_{k=1}^n (A_k - A_{k-1})b_k                               \\
                                 & = \sum_{k=1}^n A_k b_k - \sum_{k=1}^n A_k b_{k+1} + A_n b_{n+1} \\
                                 & = A_n b_{n+1} - \sum_{k=1}^n A_k (b_{k+1} - b_k).
        \end{aligned}
    \]
    For \(m < n\), we use the result above to get
    \[
        \begin{aligned}
            \sum_{k=m+1}^n a_k b_k = \sum_{k=1}^n a_k b_k - \sum_{k=1}^m a_k b_k = A_n b_{n+1} - A_m b_{m+1} - \sum_{k=m+1}^n A_k (b_{k+1} - b_k),
        \end{aligned}
    \]
    which was what we wanted. \qed

    Suppose that \(f_n, g_n: E \ra \R\). Define a partial sum of \(f_k\) as \(F_n(x) = \ds \sum_{k=1}^n f_k(x)\). From the assumption,
    \begin{itemize}
        \item There exists \(M > 0\) such that \(\abs{F_n(x)} < M\) for all \(n \in \N\).
        \item For large enough \(m \in \N\), we can make \(\abs{g_n(x)}\) arbitrarily small.
    \end{itemize}
    Now we show that the partial sums of \(\sum f_ng_n\) is a Cauchy sequence. For \(m < n\),
    \[
        \begin{aligned}
            \abs{\sum_{k=m+1}^n f_n g_n} & \overset{(\ast)}{=} \abs{F_n g_{n+1} - F_m g_{m+1} - \sum_{k=m+1}^n F_k \bigl(g_{k+1} - g_k\bigr)} \\
                                         & \leq \abs{F_n}\abs{g_{n+1}} + \abs{F_m}\abs{g_{m+1}} + \sum_{k=m+1}^n \abs{F_k} (g_k - g_{k+1})    \\
                                         & \leq M\bigl(\abs{g_{n+1}} + \abs{g_{m+1}} + \sum_{k=m+1}^n (g_k - g_{k+1})\bigr)                   \\
                                         & = M\bigl(\abs{g_{n+1}} + \abs{g_{m+1}}  + g_{m+1} - g_{n+1}\bigr) \leq 4M \abs{g_{n+1}}.
        \end{aligned}
    \]
    The variable \(x\) was omitted for the sake of brevity, and the third assumption \(g_k(x) - g_{k+1}(x) \geq 0\) was used in the second line.

    Finally, for any \(\epsilon > 0\), choose \(N \in \N\) large enough so that \(\abs{g_{n+1}(x)} < \epsilon/4M\). Then
    \[
        \abs{\sum_{k=m+1}^n f_n(x) g_n(x)} \leq 4M\abs{g_{n+1}(x)} < 4M\cdot\frac{\epsilon}{4M} = \epsilon,
    \]
    which shows that \(\sum f_n(x) g_n(x)\) converges uniformly on \(E\).
\end{enumerate}
\end{document}
